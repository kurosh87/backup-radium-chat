---
description:
globs:
alwaysApply: false
---
# Integrating a Custom LLM via OpenAI Compatibility

This document outlines the steps taken to integrate a custom Large Language Model (LLM), specifically a Llama 2 model hosted at `http://llm-inference.radium.cloud:8001`, into the `radiumchat` project using the Vercel AI SDK's OpenAI compatibility layer.

## 1. Initial Setup and Configuration

The goal was to add the custom Llama 2 model as an option alongside existing models, leveraging the pattern observed in the `reasoning-starter` sample project.

### 1.1 Environment Variables

The first step was to configure the necessary environment variables in the `.env.local` file to allow the application to connect to the custom endpoint.

```path=.env.local
# Custom Llama 2 Configuration
CUSTOM_LLM_BASE_URL="http://llm-inference.radium.cloud:8001/v1"
CUSTOM_LLM_API_KEY="empty" # Based on the curl command's 'Authorization: Bearer empty'
CUSTOM_LLM_MODEL_ID="/models/meta-llama/Llama-2-7b-chat-hf" # Specific model identifier
```

### 1.2 Backend API Route (`app/(chat)/api/chat/route.ts`)

The core logic resides in the chat API route. We modified it to handle the custom model selection:

-   **Conditional Provider Creation:** Added an `if` condition to check if the `selectedChatModel` from the request is `'custom-llama2'`.
-   **Custom OpenAI Client:** Inside the condition, use `createOpenAI` from `@ai-sdk/openai` to instantiate a new provider specifically configured for the custom endpoint:
    ```typescript
    provider = createOpenAI({
      baseURL: process.env.CUSTOM_LLM_BASE_URL,
      apiKey: process.env.CUSTOM_LLM_API_KEY, // Uses the 'empty' key from .env.local
      compatibility: 'compatible', // Crucial for non-OpenAI endpoints mimicking the API
    });
    modelIdToUse = process.env.CUSTOM_LLM_MODEL_ID; // Use the specific model ID
    ```
-   **Model Instantiation:** Use the selected `provider` (either the custom one or the default) and the determined `modelIdToUse` to get the language model instance:
    ```typescript
    const model = provider.languageModel(modelIdToUse as any);
    ```

### 1.3 Frontend UI (Model Definition)

To make the model selectable in the user interface:

-   **Locate Model Definitions:** Identified that the model dropdown (`components/model-selector.tsx`) dynamically reads models from `lib/ai/models.ts`.
-   **Add Custom Model Entry:** Added a new object to the `chatModels` array in `lib/ai/models.ts`:
    ```typescript
    // lib/ai/models.ts
    export const chatModels: Array<ChatModel> = [
      // ... other models
      {
        id: 'custom-llama2', // Matches the ID used in the backend API route
        name: 'Llama 2 (Custom)', // Display name for the UI
        description: 'Meta Llama 2 hosted locally', // Description for the UI
      },
    ];
    ```

## 2. Debugging the Local Environment Issue

After the initial setup, the custom model worked on the Vercel deployment but failed locally, showing a generic "An error occurred, please try again!" message in the UI.

### 2.1 Verifying Local Connectivity

-   **`curl` Test:** Ran the original `curl` command directly from the local machine's terminal to the custom endpoint (`http://llm-inference.radium.cloud:8001/v1/completions`).
    -   **Result:** The `curl` command succeeded, receiving a valid JSON response. This confirmed basic network connectivity and that the endpoint was responsive.

### 2.2 Enhancing Backend Error Logging

-   **Problem:** The initial backend logs only showed the API route starting successfully (e.g., `POST /api/chat 200`) but didn't reveal errors happening *during* the stream processing.
-   **Solution:** Modified `app/(chat)/api/chat/route.ts` to add `console.error` logging within the `onError` callback of `createDataStreamResponse` and in the main `catch` block.
    ```typescript
    // app/(chat)/api/chat/route.ts
    return createDataStreamResponse({
      // ... execute ...
      onError: (error) => {
        console.error('[API Chat Stream Error]', error); // Added detailed logging
        return 'Oops, an error occurred!';
      },
    });
    // ... main try/catch ...
    catch (error) {
      console.error('[API Chat Route Error]', error); // Added detailed logging
      // ... rest of catch block ...
    }
    ```

### 2.3 Identifying the Root Cause

-   **Restart and Retest:** Restarted the local development server (`pnpm run dev`) and attempted to use the custom Llama model again.
-   **Revealing Error Log:** The enhanced logging captured the specific error from the Vercel AI SDK's underlying API call:
    ```
    [API Chat Stream Error] [Error [AI_APICallError]: Bad Request] {
      ...
      responseBody: '{"object":"error","message":"\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set","type":"BadRequestError","param":null,"code":400}',
      ...
    }
    ```
-   **Diagnosis:** The custom Llama endpoint rejected the request because the Vercel AI SDK (`streamText`) was automatically including tool configuration (`tool_choice: "auto"`) in the request payload (since tools like `getWeather` were defined for other models). The custom endpoint did not support this or wasn't configured for it.

### 2.4 Implementing the Fix

-   **Disable Tools for Custom LLM:** Modified the `experimental_activeTools` option within the `streamText` call in `app/(chat)/api/chat/route.ts` to explicitly disable tools when the `custom-llama2` model is selected.
    ```typescript
    // app/(chat)/api/chat/route.ts
    const result = streamText({
      // ... other options ...
      experimental_activeTools:
        selectedChatModel === 'chat-model-reasoning' || selectedChatModel === 'custom-llama2' // Added custom-llama2 here
          ? [] // Ensure tools are disabled
          : [ /* list of tools for other models */ ],
      // ... rest of options ...
    });
    ```

## 3. Result

After restarting the development server with the fix in place, selecting "Llama 2 (Custom)" and sending messages successfully streamed responses from the custom endpoint without errors.
